# Skill Evaluation Criteria

Reference — load when evaluating skill quality.

## Quick Assessment (from search results)

Before recommending, check:
- **Description clarity** — Does it explain WHAT and WHEN?
- **Author reputation** — Known? Multiple skills?
- **Download count** — Popular skills are often better maintained
- **Last update** — Recent = active maintenance

## Structure Quality (from ClawHub page)

View skill on ClawHub to check structure:

✅ Good signs:
- SKILL.md under 80 lines
- Auxiliary files for details
- Clear section organization
- References to when to load each file

❌ Red flags:
- Wall of text in SKILL.md
- No progressive disclosure
- Explains obvious concepts
- README, CHANGELOG clutter

## Instruction Quality

✅ Good signs:
- Imperative voice ("Do X")
- Actionable instructions
- Clear triggers
- Examples when helpful

❌ Red flags:
- Passive/verbose ("Users should consider...")
- Theory without action
- Vague guidance
- Over-explanation

## Fit Assessment

Ask:
- Does it solve the ACTUAL need?
- Is it too broad or too narrow?
- Does it conflict with installed skills?
- Is it worth the context cost?

## Scoring (Mental Model)

Rate 1-5 on:
- **Relevance** — How well does it match the need?
- **Quality** — How well is it built?
- **Value** — Is it worth the tokens?

Only recommend if scores ≥3 on all.

## Reporting to User

When recommending:
> "Found `[skill]` — [one line what it does]. [Quality note if relevant]. Want me to install it?"

When not recommending:
> "Found `[skill]` but [concern]. [Alternative suggestion or ask if want anyway]."
